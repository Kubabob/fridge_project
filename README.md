# Food Recognition System | System Rozpoznawania ≈ªywno≈õci

**Language | Jƒôzyk**: [English](#english) | [Polski](#polski)

---

<a name="english"></a>
# Food Recognition System
![Image of recognized blueberries](calculations/evaluation_results/1750876467.7753909.png)

## üìù Project Overview

A computer vision system for food recognition using YOLO (You Only Look Once) segmentation models. This project implements real-time food detection and segmentation capable of identifying items from nearly 500 different food classes.

## üîë Key Features

- Real-time food object detection and segmentation
- Support for nearly 500 food classes
- Instance segmentation with pixel-level accuracy
- Optimized for performance on consumer hardware

## üõ†Ô∏è Technologies

- **YOLO11n-seg**: Lightweight segmentation model from the YOLOv8 family
- **PyTorch**: Deep learning framework
- **COCO Dataset Format**: For initial data preparation
- **GPU Acceleration**: For efficient model training

## üìä Dataset

The system was trained on the food-recognition-2022 dataset containing:
- **Training set**: 76,491 annotations across 498 food classes
- **Validation set**: 1,830 annotations across 498 food classes

### Data Preparation

We developed a pipeline to convert the original COCO JSON format into the YOLO-compatible format:
- Images resized to 640√ó640
- Labels converted to YOLO format (class_id, x_center, y_center, width, height)
- Created dataset.yaml with all class names

## üß† Model Architecture & Training

The model is based on YOLOv8, pre-trained on the COCO dataset (80 common object classes) and fine-tuned on our food dataset. We utilized transfer learning to adapt the model to the food domain.

### Training Parameters

- **Model**: YOLO11n-seg (nano segmentation variant)
- **Epochs**: 40
- **Learning Rate**: Dynamic with warmup and decay
- **Loss Components**:
  - Box Loss: For bounding box regression
  - Classification Loss: For food class identification
  - DFL (Distribution Focal Loss): For better bbox boundary prediction
  - Segmentation Loss: For pixel-level mask prediction
- **Regularization**: Weight decay to prevent overfitting

## üìà Results & Performance Analysis

The model completed training after 40 epochs. While not optimal, it demonstrates the capabilities of the approach.

### High-Performance Classes (F1 ‚âà 0.995)
- Fruit salad
- Croissant
- Popcorn salted
- Chorizo
- Wine rose

### Challenging Classes (F1 < 0.1)
- Chocolate (0.103)
- Basil (0.016)
- Nectarine (0.0815)
- Cottage cheese (0.0822)
- Ham (0.0148)

### Performance Metrics

- **mAP (mean Average Precision)**: ~0.2 (competitive with early YOLO implementations)
- **Precision**: High at maximum confidence threshold (~0.95)
- **Recall**: Generally lower than precision, indicating conservative predictions
- **Train-validation difference**: Small, suggesting good generalization

### Observations

- The model demonstrates better precision than recall, meaning it's more likely to miss objects than to give false positives
- Learning continues through all epochs, suggesting potential for improvement with extended training
- Box Loss and Segmentation Loss remained the highest, indicating challenges with precise localization and shape delineation
- Classification performance was relatively stronger than localization performance

## üöÄ Usage

```python
# Example code for inference
from ultralytics import YOLO

# Load the trained model
model = YOLO("path/to/trained/model.pt")

# Perform inference on an image
results = model("path/to/food/image.jpg")

# Display results
results[0].show()
```

## üì¶ Installation

This project uses [uv](https://github.com/astral-sh/uv) as its Python package manager for fast, reliable dependency management.

### Prerequisites

- Python 3.13 or higher
- Git

### Installation Steps

1. **Install uv** (if not already installed):

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

2. **Clone the repository**:

```bash
git clone https://github.com/yourusername/food-recognition-system.git
cd food-recognition-system
```

3. **Create a virtual environment and install dependencies**:

```bash
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -e .
```

4. **Verify installation**:

```bash
python -c "from ultralytics import YOLO; print('Installation successful!')"
```

## üîÆ Future Improvements

- Extended training with more epochs
- Class balancing to improve performance on challenging classes
- Data augmentation to enhance generalization
- Model architecture experiments with larger variants (YOLO11m-seg, YOLO11l-seg)
- Hyperparameter optimization, particularly for box and segmentation loss components

## üë• Contributors

This project was developed as part of academic coursework of Jakub Bo≈ºek and Karolina Klemenska at University of Gda≈Ñsk.

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


---

<a name="polski"></a>
# System Rozpoznawania ≈ªywno≈õci
![Zdjƒôcie z rozpoznanymi bor√≥wkami](calculations/evaluation_results/1750876467.7753909.png)

## üìù PrzeglƒÖd Projektu

System wizji komputerowej do rozpoznawania ≈ºywno≈õci wykorzystujƒÖcy modele segmentacji YOLO (You Only Look Once). Projekt implementuje wykrywanie i segmentacjƒô ≈ºywno≈õci w czasie rzeczywistym, zdolnƒÖ do identyfikacji produkt√≥w z niemal 500 r√≥≈ºnych klas ≈ºywno≈õci.

## üîë Kluczowe Funkcje

- Wykrywanie i segmentacja obiekt√≥w ≈ºywno≈õciowych w czasie rzeczywistym
- Obs≈Çuga prawie 500 klas ≈ºywno≈õci
- Segmentacja instancji z dok≈Çadno≈õciƒÖ na poziomie pikseli
- Zoptymalizowany pod kƒÖtem wydajno≈õci na sprzƒôcie konsumenckim

## üõ†Ô∏è Technologie

- **YOLO11n-seg**: Lekki model segmentacji z rodziny YOLOv8
- **PyTorch**: Framework do g≈Çƒôbokiego uczenia
- **Format Danych COCO**: Do wstƒôpnego przygotowania danych
- **Akceleracja GPU**: Do efektywnego treningu modelu

## üìä Zbi√≥r Danych

System zosta≈Ç wytrenowany na zbiorze danych food-recognition-2022 zawierajƒÖcym:
- **Zbi√≥r treningowy**: 76 491 adnotacji w 498 klasach ≈ºywno≈õci
- **Zbi√≥r walidacyjny**: 1 830 adnotacji w 498 klasach ≈ºywno≈õci

### Przygotowanie Danych

Opracowali≈õmy pipeline do konwersji oryginalnego formatu JSON COCO na format kompatybilny z YOLO:
- Obrazy przeskalowane do 640√ó640
- Etykiety przekonwertowane do formatu YOLO (id_klasy, x_≈õrodek, y_≈õrodek, szeroko≈õƒá, wysoko≈õƒá)
- Utworzono plik dataset.yaml ze wszystkimi nazwami klas

## üß† Architektura Modelu i Trening

Model bazuje na YOLOv8, wstƒôpnie wytrenowanym na zbiorze danych COCO (80 typowych klas obiekt√≥w) i dostrojonym na naszym zbiorze danych ≈ºywno≈õci. Wykorzystali≈õmy transfer learning do adaptacji modelu do domeny ≈ºywno≈õci.

### Parametry Treningu

- **Model**: YOLO11n-seg (wariant nano segmentacji)
- **Epoki**: 40
- **Wsp√≥≈Çczynnik uczenia**: Dynamiczny z rozgrzewkƒÖ i zanikaniem
- **Komponenty funkcji straty**:
  - Box Loss: Do regresji bounding box√≥w
  - Classification Loss: Do identyfikacji klas ≈ºywno≈õci
  - DFL (Distribution Focal Loss): Do lepszego przewidywania granic bbox√≥w
  - Segmentation Loss: Do przewidywania masek na poziomie pikseli
- **Regularyzacja**: Zanikanie wag, aby zapobiec przeuczeniu

## üìà Analiza Wynik√≥w i Wydajno≈õci

Model zako≈Ñczy≈Ç trening po 40 epokach. Choƒá nie jest optymalny, demonstruje mo≈ºliwo≈õci podej≈õcia.

### Klasy o Wysokiej Wydajno≈õci (F1 ‚âà 0.995)
- Sa≈Çatka owocowa
- Croissant
- Popcorn solony
- Chorizo
- Wino r√≥≈ºowe

### Trudne Klasy (F1 < 0.1)
- Czekolada (0.103)
- Bazylia (0.016)
- Nektarynka (0.0815)
- Twar√≥g (0.0822)
- Szynka (0.0148)

### Metryki Wydajno≈õci

- **mAP (≈õrednia Precyzja)**: ~0.2 (konkurencyjna wobec wczesnych implementacji YOLO)
- **Precyzja**: Wysoka przy maksymalnym progu pewno≈õci (~0.95)
- **Czu≈Ço≈õƒá**: Og√≥lnie ni≈ºsza ni≈º precyzja, wskazujƒÖc na konserwatywne przewidywania
- **R√≥≈ºnica trening-walidacja**: Ma≈Ça, sugerujƒÖca dobrƒÖ generalizacjƒô

### Obserwacje

- Model wykazuje lepszƒÖ precyzjƒô ni≈º czu≈Ço≈õƒá, co oznacza, ≈ºe jest bardziej sk≈Çonny do pominiƒôcia obiekt√≥w ni≈º do fa≈Çszywych alarm√≥w
- Uczenie kontynuowane przez wszystkie epoki, sugerujƒÖce potencja≈Ç do poprawy przy przed≈Çu≈ºonym treningu
- Box Loss i Segmentation Loss pozosta≈Çy najwy≈ºsze, wskazujƒÖc na wyzwania z precyzyjnƒÖ lokalizacjƒÖ i delineacjƒÖ kszta≈Çtu
- Wydajno≈õƒá klasyfikacji by≈Ça stosunkowo silniejsza ni≈º wydajno≈õƒá lokalizacji

## üöÄ U≈ºycie

```python
# Przyk≈Çadowy kod do wnioskowania
from ultralytics import YOLO

# Za≈Çaduj wytrenowany model
model = YOLO("≈õcie≈ºka/do/wytrenowanego/modelu.pt")

# Wykonaj wnioskowanie na obrazie
results = model("≈õcie≈ºka/do/obrazu/≈ºywno≈õci.jpg")

# Wy≈õwietl wyniki
results[0].show()
```

## üì¶ Instalacja

Ten projekt wykorzystuje [uv](https://github.com/astral-sh/uv) jako mened≈ºer pakiet√≥w Python dla szybkiego i niezawodnego zarzƒÖdzania zale≈ºno≈õciami.

### Wymagania wstƒôpne

- Python 3.13 lub nowszy
- Git

### Kroki instalacji

1. **Zainstaluj uv** (je≈õli nie jest jeszcze zainstalowany):

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

2. **Sklonuj repozytorium**:

```bash
git clone https://github.com/twojnazwauzytkownika/system-rozpoznawania-zywnosci.git
cd system-rozpoznawania-zywnosci
```

3. **Utw√≥rz ≈õrodowisko wirtualne i zainstaluj zale≈ºno≈õci**:

```bash
uv venv
source .venv/bin/activate  # W Windows: .venv\Scripts\activate
uv pip install -e .
```

4. **Zweryfikuj instalacjƒô**:

```bash
python -c "from ultralytics import YOLO; print('Instalacja zako≈Ñczona sukcesem!')"
```

## üîÆ Przysz≈Çe Ulepszenia

- Rozszerzony trening z wiƒôkszƒÖ liczbƒÖ epok
- Zbalansowanie klas w celu poprawy wydajno≈õci dla trudnych klas
- Augmentacja danych w celu zwiƒôkszenia generalizacji
- Eksperymenty z architekturƒÖ modelu z wiƒôkszymi wariantami (YOLO11m-seg, YOLO11l-seg)
- Optymalizacja hiperparametr√≥w, szczeg√≥lnie dla komponent√≥w Box Loss i Segmentation Loss

## üë• Wsp√≥≈Çtw√≥rcy

Projekt zosta≈Ç opracowany jako czƒô≈õƒá pracy akademickiej Jakuba Bo≈ºka i Karoliny Klemenskiej na Uniwersytecie Gda≈Ñskim.

## üìÑ Licencja

Ten projekt jest licencjonowany na mocy licencji MIT - zobacz plik [LICENSE](LICENSE) dla szczeg√≥≈Ç√≥w.

