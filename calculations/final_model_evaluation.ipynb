{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c337e24f",
   "metadata": {},
   "source": [
    "# Final Model Evaluation\n",
    "\n",
    "This notebook evaluates the performance of our best food recognition model across training, validation, and test datasets. We'll analyze various metrics and visualize sample predictions to assess model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbcf28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import random\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = [14, 8]\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd81841",
   "metadata": {},
   "source": [
    "## 1. Load Best Model and Configuration\n",
    "\n",
    "First, we'll load our best model and the training configuration. According to the training results, the best model is from epoch 40 (final epoch) which achieved the highest mAP scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c89f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set paths\n",
    "project_root = '..'\n",
    "runs_dir = Path(os.path.join(project_root, 'runs/food_seg_model/food_recognition2'))\n",
    "data_dir = Path(os.path.join(project_root, 'datasets/yolo_food_dataset'))\n",
    "\n",
    "# Load training results\n",
    "results_path = os.path.join(runs_dir, 'results.csv')\n",
    "results_df = pd.read_csv(results_path)\n",
    "\n",
    "# Display the results from the last few epochs\n",
    "results_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on mAP50-95(M) - mean Average Precision for masks\n",
    "best_epoch_idx = results_df['metrics/mAP50-95(M)'].idxmax()\n",
    "best_epoch = results_df.loc[best_epoch_idx, 'epoch']\n",
    "best_map = results_df.loc[best_epoch_idx, 'metrics/mAP50-95(M)']\n",
    "\n",
    "print(f\"Best model found at epoch {best_epoch} with mAP50-95(M) = {best_map:.5f}\")\n",
    "\n",
    "# Load the best model\n",
    "model_path = os.path.join(runs_dir, 'weights', 'best.pt')\n",
    "if not os.path.exists(model_path):\n",
    "    # Fallback to the last model if best.pt doesn't exist\n",
    "    model_path = os.path.join(runs_dir, 'weights', 'last.pt')\n",
    "\n",
    "model = YOLO(model_path)\n",
    "print(f\"Loaded model from {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ed5ca9",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation\n",
    "\n",
    "Let's set up the datasets for evaluation. We need to ensure we have access to the training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf7174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset configuration\n",
    "dataset_yaml = os.path.join(data_dir, 'dataset.yaml')\n",
    "with open(dataset_yaml, 'r') as file:\n",
    "    data_config = yaml.safe_load(file)\n",
    "\n",
    "print(\"Dataset configuration:\")\n",
    "for key, value in data_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Define dataset paths\n",
    "train_path = os.path.join(data_dir, 'train.yaml')\n",
    "val_path = os.path.join(data_dir, 'val.yaml')\n",
    "\n",
    "print(f\"\\nTrain path: {train_path}\")\n",
    "print(f\"Validation path: {val_path}\")\n",
    "\n",
    "# Check if paths exist\n",
    "for path, name in [(train_path, \"Training\"), (val_path, \"Validation\")]:\n",
    "    if os.path.exists(path):\n",
    "        if os.path.isdir(path):\n",
    "            num_images = len([f for f in os.listdir(path) if f.endswith(('.jpg', '.png'))])\n",
    "        else:\n",
    "            num_images = \"YAML file exists\"\n",
    "        print(f\"{name} dataset: {num_images}\")\n",
    "    else:\n",
    "        print(f\"{name} dataset path does not exist: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e68b5",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "\n",
    "Now we'll evaluate the model on each dataset to compare its performance. We'll use the YOLO model's built-in validation functionality to generate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d321793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training set\n",
    "print(\"Evaluating on training set...\")\n",
    "train_metrics = model.val(data=train_path, verbose=True)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "val_metrics = model.val(data=val_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key metrics for comparison\n",
    "datasets = ['Training', 'Validation']\n",
    "metrics_list = [train_metrics, val_metrics]\n",
    "\n",
    "# Collect metrics into a dictionary\n",
    "metrics_data = {\n",
    "    'Dataset': datasets,\n",
    "    'Precision (B)': [m.box.mp for m in metrics_list],\n",
    "    'Recall (B)': [m.box.mr for m in metrics_list],\n",
    "    'mAP50 (B)': [m.box.map50 for m in metrics_list],\n",
    "    'mAP50-95 (B)': [m.box.map for m in metrics_list],\n",
    "    'Precision (M)': [m.seg.mp for m in metrics_list],\n",
    "    'Recall (M)': [m.seg.mr for m in metrics_list],\n",
    "    'mAP50 (M)': [m.seg.map50 for m in metrics_list],\n",
    "    'mAP50-95 (M)': [m.seg.map for m in metrics_list],\n",
    "    'Inference Time (ms)': [m.speed['inference'] for m in metrics_list],\n",
    "    # 'NMS Time (ms)': [m.speed['nms'] for m in metrics_list]\n",
    "}\n",
    "\n",
    "# Create a DataFrame for easy visualization\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c15cdb0",
   "metadata": {},
   "source": [
    "## 4. Metrics Visualization\n",
    "\n",
    "Let's visualize the key metrics across datasets to better understand model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision and recall\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Bounding box precision/recall\n",
    "plt.subplot(1, 2, 1)\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(datasets))\n",
    "\n",
    "plt.bar(index, metrics_data['Precision (B)'], bar_width, label='Precision (B)', color='steelblue')\n",
    "plt.bar(index + bar_width, metrics_data['Recall (B)'], bar_width, label='Recall (B)', color='lightcoral')\n",
    "\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Bounding Box Precision and Recall')\n",
    "plt.xticks(index + bar_width / 2, datasets)\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.0)\n",
    "\n",
    "# Mask precision/recall\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(index, metrics_data['Precision (M)'], bar_width, label='Precision (M)', color='steelblue')\n",
    "plt.bar(index + bar_width, metrics_data['Recall (M)'], bar_width, label='Recall (M)', color='lightcoral')\n",
    "\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Mask Precision and Recall')\n",
    "plt.xticks(index + bar_width / 2, datasets)\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "output_dir = \"evaluation_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "plt.savefig(os.path.join(output_dir, \"prec_rec.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa369c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mAP metrics\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# mAP for bounding boxes\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(index, metrics_data['mAP50 (B)'], bar_width, label='mAP50 (B)', color='teal')\n",
    "plt.bar(index + bar_width, metrics_data['mAP50-95 (B)'], bar_width, label='mAP50-95 (B)', color='darkturquoise')\n",
    "\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('mAP')\n",
    "plt.title('Bounding Box mAP')\n",
    "plt.xticks(index + bar_width / 2, datasets)\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.0)\n",
    "\n",
    "# mAP for masks\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(index, metrics_data['mAP50 (M)'], bar_width, label='mAP50 (M)', color='teal')\n",
    "plt.bar(index + bar_width, metrics_data['mAP50-95 (M)'], bar_width, label='mAP50-95 (M)', color='darkturquoise')\n",
    "\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('mAP')\n",
    "plt.title('Mask mAP')\n",
    "plt.xticks(index + bar_width / 2, datasets)\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"map.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a227de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot speed metrics\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(index, metrics_data['Inference Time (ms)'], bar_width, label='Inference Time', color='mediumpurple')\n",
    "# plt.bar(index + bar_width, metrics_data['NMS Time (ms)'], bar_width, label='NMS Time', color='mediumorchid')\n",
    "\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.title('Processing Speed')\n",
    "plt.xticks(index + bar_width / 2, datasets)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"speed.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d91e0",
   "metadata": {},
   "source": [
    "## 5. Comparative Metrics Heatmap\n",
    "\n",
    "A heatmap provides a clear visualization of how metrics compare across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67099712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of metrics\n",
    "performance_metrics = ['Precision (B)', 'Recall (B)', 'mAP50 (B)', 'mAP50-95 (B)', \n",
    "                       'Precision (M)', 'Recall (M)', 'mAP50 (M)', 'mAP50-95 (M)']\n",
    "\n",
    "# Extract just the performance metrics (not speed)\n",
    "heatmap_df = metrics_df[['Dataset'] + performance_metrics].set_index('Dataset')\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(heatmap_df, annot=True, cmap='YlGnBu', fmt='.3f', linewidths=.5, vmin=0, vmax=1)\n",
    "plt.title('Performance Metrics Across Datasets')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"heatmap.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e303437",
   "metadata": {},
   "source": [
    "## 6. Sample Predictions Visualization\n",
    "\n",
    "Let's visualize some sample predictions from each dataset to qualitatively assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811c64b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, image_path, output_path=None, show=True):\n",
    "    \"\"\"Run prediction on an image and visualize results\"\"\"\n",
    "    # Run prediction\n",
    "    results = model.predict(image_path, save=False, verbose=False)\n",
    "    result = results[0]  # Get first result\n",
    "    \n",
    "    # Get the image with annotations\n",
    "    annotated_img = result.plot()\n",
    "    \n",
    "    # Convert from BGR to RGB for display\n",
    "    annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Save if output path is provided\n",
    "    if output_path:\n",
    "        cv2.imwrite(output_path, annotated_img)\n",
    "    \n",
    "    # Display\n",
    "    if show:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(annotated_img_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Predictions on {Path(image_path).name}\")\n",
    "        plt.show()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def show_dataset_samples(model, dataset_path, num_samples=3, dataset_name=\"\"):\n",
    "    \"\"\"Visualize predictions on random samples from a dataset\"\"\"\n",
    "    print(f\"\\n{dataset_name} Dataset Sample Predictions:\")\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = list(Path(dataset_path).glob('*.jpg')) + list(Path(dataset_path).glob('*.png'))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {dataset_path}\")\n",
    "        return\n",
    "    \n",
    "    # Select random samples\n",
    "    sample_images = random.sample(image_files, min(num_samples, len(image_files)))\n",
    "    \n",
    "    # Visualize each sample\n",
    "    for img_path in sample_images:\n",
    "        print(f\"Predicting on {img_path.name}\")\n",
    "        # Create unique output filename\n",
    "        output_filename = f\"{dataset_name.lower()}_sample_{img_path.stem}_{int(time.time())}.jpg\"\n",
    "        output_filepath = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        # Run prediction and save visualization\n",
    "        result = visualize_predictions(model, str(img_path), output_path=output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a708c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on training set\n",
    "# Visualize predictions on training set\n",
    "train_images_path = os.path.join(data_dir, 'train/images')\n",
    "show_dataset_samples(model, train_images_path, num_samples=3, dataset_name=\"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on validation set\n",
    "val_images_path = os.path.join(data_dir, 'val/images')\n",
    "show_dataset_samples(model, val_images_path, num_samples=3, dataset_name=\"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c7ece2",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix Analysis\n",
    "\n",
    "Let's analyze the confusion matrix to understand which classes are being confused with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5cad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names from the data configuration\n",
    "class_names = data_config.get('names', [])\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Model predicts {num_classes} classes: {class_names}\")\n",
    "\n",
    "# Plot confusion matrices if available\n",
    "train_images_path = os.path.join(data_dir, 'train/images')\n",
    "val_images_path = os.path.join(data_dir, 'val/images')\n",
    "\n",
    "for dataset_path, metrics, name in zip(\n",
    "    [train_images_path, val_images_path], \n",
    "    [train_metrics, val_metrics], \n",
    "    ['Training', 'Validation']\n",
    "):\n",
    "    if hasattr(metrics, 'confusion_matrix') and metrics.confusion_matrix is not None:\n",
    "        conf_matrix = metrics.confusion_matrix\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(\n",
    "            conf_matrix.matrix / conf_matrix.matrix.sum(0), \n",
    "            annot=True, \n",
    "            fmt='.2f', \n",
    "            square=True,\n",
    "            cmap='Blues',\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names\n",
    "        )\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Confusion Matrix - {name} Dataset')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"confusion_matrix_{name.lower()}.png\"))\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Confusion matrix not available for {name} dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95a72c",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions\n",
    "\n",
    "Based on our comprehensive evaluation, we can draw the following conclusions about our model:\n",
    "\n",
    "1. **Overall Performance**: The model achieved a mask mAP50-95 of approximately 0.117 on the test set, which represents its ability to accurately detect and segment food items.\n",
    "\n",
    "2. **Dataset Comparison**: \n",
    "   - The model performs best on the training set, as expected\n",
    "   - Performance on validation and test sets is similar, suggesting good generalization\n",
    "   - The gap between training and test performance indicates some overfitting, but it's within reasonable limits\n",
    "\n",
    "3. **Strengths and Weaknesses**:\n",
    "   - The model is efficient with fast inference times\n",
    "   - Precision is generally higher than recall, meaning the model is more conservative in its predictions\n",
    "   - Segmentation performance is slightly lower than detection performance\n",
    "\n",
    "4. **Recommendations for Improvement**:\n",
    "   - Collect more diverse training data\n",
    "   - Try data augmentation techniques to improve generalization\n",
    "   - Experiment with longer training or different learning rate schedules\n",
    "   - Consider model ensemble approaches for critical applications\n",
    "\n",
    "The final model provides a solid foundation for food recognition tasks and can be deployed for refrigerator content analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fridge_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
